<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><meta name="baidu-site-verification"><title>大数据采集、清洗、处理：使用MapReduce进行离线数据分析---大数据 | 艾利克斯工作室</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="艾利克斯工作室" type="application/atom+xml">
</head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><body><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">LITREILY</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">艾利克斯工作室</a></h1></div><p class="m-desc">路在脚下<br>The road is under your feet.<br>目前专注于深度学习 声学算法等AI领域</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/Cooperation/">合作</a></li><li><span class="dot">●</span><a href="/Case/">案例</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">大数据采集、清洗、处理：使用MapReduce进行离线数据分析---大数据</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2020/06/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E3%80%81%E6%B8%85%E6%B4%97%E3%80%81%E5%A4%84%E7%90%86%EF%BC%9A%E4%BD%BF%E7%94%A8MapReduce%E8%BF%9B%E8%A1%8C%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A4%A7%E6%95%B0%E6%8D%AE/">2020-06-01</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p>在互联网应用中，不管是哪一种处理方式，其基本的数据来源都是日志数据<a id="more"></a>，例如对于web应用来说，则可能是用户的访问日志、用户的点击日志等。<br>如果对于数据的分析结果在时间上有比较严格的要求，则可以采用在线处理的方式来对数据进行分析，如使用Spark、Storm等进行处理。比较贴切的一个例子是天猫双十一的成交额，在其展板上，我们看到交易额是实时动态进行更新的，对于这种情况，则需要采用在线处理。<br>当然，如果只是希望得到数据的分析结果，对处理的时间要求不严格，就可以采用离线处理的方式，比如我们可以先将日志数据采集到HDFS中，之后再进一步使用MapReduce、Hive等来对数据进行分析，这也是可行的。<br>这份文档主要分享对某个电商网站产生的用户访问日志（access.log）进行离线处理与分析的过程，基于MapReduce的处理方式，最后会统计出某一天不同省份访问该网站的uv与pv。</p>
<h1 id="大数据处理的常用方法"><a href="#大数据处理的常用方法" class="headerlink" title="大数据处理的常用方法"></a>大数据处理的常用方法</h1><p>大数据处理目前比较流行的是两种方法，一种是离线处理，一种是在线处理。</p>
<h1 id="生产场景与需求"><a href="#生产场景与需求" class="headerlink" title="生产场景与需求"></a>生产场景与需求</h1><p>在我们的场景中，Web应用的部署是如下的架构：<br>即比较典型的Nginx负载均衡+KeepAlive高可用集群架构，在每台Web服务器上，都会产生用户的访问日志，业务需求方给出的日志格式如下：  </p>
<pre><code>1001    211.167.248.22  eecf0780-2578-4d77-a8d6-e2225e8b9169    40604   1       GET /top HTTP/1.0       408     null      null    1523188122767
1003    222.68.207.11   eecf0780-2578-4d77-a8d6-e2225e8b9169    20202   1       GET /tologin HTTP/1.1   504     null      Mozilla/5.0 (Windows; U; Windows NT 5.1)Gecko/20070309 Firefox/2.0.0.3  1523188123267
1001    61.53.137.50    c3966af9-8a43-4bda-b58c-c11525ca367b    0       1       GET /update/pass HTTP/1.0       302       null    null    1523188123768
1000    221.195.40.145  1aa3b538-2f55-4cd7-9f46-6364fdd1e487    0       0       GET /user/add HTTP/1.1  200     null      Mozilla/4.0 (compatible; MSIE 7.0; Windows NT5.2)       1523188124269
1000    121.11.87.171   8b0ea90a-77a5-4034-99ed-403c800263dd    20202   1       GET /top HTTP/1.0       408     null      Mozilla/5.0 (Windows; U; Windows NT 5.1)Gecko/20070803 Firefox/1.5.0.12 1523188120263</code></pre><p>其每个字段的说明如下：  </p>
<pre><code>appid ip mid userid login_type request status http_referer user_agent time
其中：
appid包括：web:1000,android:1001,ios:1002,ipad:1003
mid:唯一的id此id第一次会种在浏览器的cookie里。如果存在则不再种。作为浏览器唯一标示。移动端或者pad直接取机器码。
login_type：登录状态，0未登录、1：登录用户
request：类似于此种 &quot;GET /userList HTTP/1.1&quot;
status：请求的状态主要有：200 ok、404 not found、408 Request Timeout、500 Internal Server Error、504 Gateway Timeout等
http_referer：请求该url的上一个url地址。
user_agent：浏览器的信息，例如：&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36&quot;
time：时间的long格式：1451451433818。</code></pre><p>根据给定的时间范围内的日志数据，现在业务方有如下需求：<br>统计出每个省每日访问的PV、UV。  </p>
<h1 id="数据采集：获取原生数据"><a href="#数据采集：获取原生数据" class="headerlink" title="数据采集：获取原生数据"></a>数据采集：获取原生数据</h1><p>对于用户访问日志的采集，使用的是Flume，并且会将采集的数据保存到HDFS中，其架构思路如下：  </p>
<pre><code>不同的Web Server上都会部署一个Agent用于该Server上日志数据的采集，之后，不同Web Server的Flume Agent采集的日志数据会下沉到另外一个被称为Flume Consolidation Agent（聚合Agent）的Flume Agent上，该Flume Agent的数据落地方式为输出到HDFS。</code></pre><p>在我们的HDFS中，可以查看到其采集的日志：<br><img src="/images/hadoop.png"></p>
<p>后面我们的工作正是要基于Flume采集到HDFS中的数据做离线处理与分析。</p>
<h1 id="数据清洗："><a href="#数据清洗：" class="headerlink" title="数据清洗："></a>数据清洗：</h1><p>将不规整数据转化为规整数据</p>
<h1 id="数据清洗目的"><a href="#数据清洗目的" class="headerlink" title="数据清洗目的"></a>数据清洗目的</h1><p>刚刚采集到HDFS中的原生数据，我们也称为不规整数据，即目前来说，该数据的格式还无法满足我们对数据处理的基本要求，需要对其进行预处理，转化为我们后面工作所需要的较为规整的数据，所以这里的数据清洗，其实指的就是对数据进行基本的预处理，以方便我们后面的统计分析，所以这一步并不是必须的，需要根据不同的业务需求来进行取舍，只是在我们的场景中需要对数据进行一定的处理。</p>
<h1 id="数据清洗方案"><a href="#数据清洗方案" class="headerlink" title="数据清洗方案"></a>数据清洗方案</h1><p>原来的日志数据格式是如下的：</p>
<pre><code>appid ip mid userid login_type request status http_referer user_agent time
其中：
appid包括：web:1000,android:1001,ios:1002,ipad:1003
mid:唯一的id此id第一次会种在浏览器的cookie里。如果存在则不再种。作为浏览器唯一标示。移动端或者pad直接取机器码。
login_type：登录状态，0未登录、1：登录用户
request：类似于此种 &quot;GET /userList HTTP/1.1&quot;
status：请求的状态主要有：200 ok、404 not found、408 Request Timeout、500 Internal Server Error、504 Gateway Timeout等
http_referer：请求该url的上一个url地址。
user_agent：浏览器的信息，例如：&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36&quot;
time：时间的long格式：1451451433818。</code></pre><p>但是如果需要按照省份来统计uv、pv，其所包含的信息还不够，我们需要对这些数据做一定的预处理，比如需要，对于其中包含的IP信息，我们需要将其对应的IP信息解析出来；为了方便我们的其它统计，我们也可以将其request信息解析为method、 request_url、 http_version等，所以按照上面的分析，我们希望预处理之后的日志数据包含如下的数据字段：  </p>
<pre><code>appid;  
ip;
//通过ip来衍生出来的字段 province和city
province;
city;

mid;      
userId;    
loginType; 
request; 
//通过request 衍生出来的字段 method request_url http_version
method;
requestUrl;
httpVersion;

status;          
httpReferer; 
userAgent;   
//通过userAgent衍生出来的字段，即用户的浏览器信息
browser;

time;</code></pre><p>即在原来的基础上，我们增加了其它新的字段，如province、city等。  </p>
<p>我们采用MapReduce来对数据进行预处理，预处理之后的结果，我们也是保存到HDFS中，即采用如下的架构：<br><img src="/images/hadoop5.png"></p>
<h1 id="数据清洗过程："><a href="#数据清洗过程：" class="headerlink" title="数据清洗过程："></a>数据清洗过程：</h1><p>MapReduce程序编写<br>数据清洗的过程主要是编写MapReduce程序，而MapReduce程序的编写又分为写Mapper、Reducer、Job三个基本的过程。但是在我们这个案例中，要达到数据清洗的目的，实际上只需要Mapper就可以了，并不需要Reducer，原因很简单，我们只是预处理数据，在Mapper中就已经可以对数据进行处理了，其输出的数据并不需要进一步经过Redcuer来进行汇总处理。  </p>
<p>所以下面就直接编写Mapper和Job的程序代码:<br>AccessLogCleanMapper</p>
<pre><code>package cn.xpleaf.dataClean.mr.mapper;

import cn.xpleaf.dataClean.mr.writable.AccessLogWritable;
import cn.xpleaf.dataClean.utils.JedisUtil;
import cn.xpleaf.dataClean.utils.UserAgent;
import cn.xpleaf.dataClean.utils.UserAgentUtil;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Logger;
import redis.clients.jedis.Jedis;

import java.io.IOException;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Date;

/**
 * access日志清洗的主要mapper实现类
 * 原始数据结构：
 * appid ip mid userid login_tpe request status http_referer user_agent time ---&gt; 10列内容
 * 清洗之后的结果：
 * appid ip province city mid userid login_type request method request_url http_version status http_referer user_agent browser yyyy-MM-dd HH:mm:ss
 */
public class AccessLogCleanMapper extends Mapper&lt;LongWritable, Text, NullWritable, Text&gt; {

    private Logger logger;
    private String[] fields;

    private String appid;      //数据来源 web:1000,android:1001,ios:1002,ipad:1003
    private String ip;
    //通过ip来衍生出来的字段 province和city
    private String province;
    private String city;

    private String mid;      //mid:唯一的id此id第一次会种在浏览器的cookie里。如果存在则不再种。作为浏览器唯一标示。移动端或者pad直接取机器码。
    private String userId;     //用户id
    private String loginType; //登录状态，0未登录、1：登录用户
    private String request; //类似于此种 &quot;GET userList HTTP/1.1&quot;
    //通过request 衍生出来的字段 method request_url http_version
    private String method;
    private String requestUrl;
    private String httpVersion;

    private String status;          //请求的状态主要有：200 ok、/404 not found、408 Request Timeout、500 Internal Server Error、504 Gateway Timeout等
    private String httpReferer; //请求该url的上一个url地址。
    private String userAgent;   //浏览器的信息，例如：&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36&quot;
    //通过userAgent来获取对应的浏览器
    private String browser;

    //private long time; //action对应的时间戳
    private String time;//action对应的格式化时间yyyy-MM-dd HH:mm:ss

    private DateFormat df;
    private Jedis jedis;

    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
        logger = Logger.getLogger(AccessLogCleanMapper.class);
        df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);
        jedis = JedisUtil.getJedis();
    }

    /**
     * appid ip mid userid login_tpe request status http_referer user_agent time ---&gt; 10列内容
     * ||
     * ||
     * appid ip province city mid userid login_type request method request_url http_version status http_referer user_agent browser yyyy-MM-dd HH:mm:ss
     */
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        fields = value.toString().split(&quot;\t&quot;);
        if (fields == null || fields.length != 10) { // 有异常数据
            return;
        }
        // 因为所有的字段没有进行特殊操作，只是文本的输出，所以没有必要设置特定类型，全部设置为字符串即可，
        // 这样在做下面的操作时就可以省去类型的转换，但是如果对数据的合法性有严格的验证的话，则要保持类型的一致
        appid = fields[0];
        ip = fields[1];
        // 解析IP
        if (ip != null) {
            String ipInfo = jedis.hget(&quot;ip_info&quot;, ip);
            province = ipInfo.split(&quot;\t&quot;)[0];
            city = ipInfo.split(&quot;\t&quot;)[1];
        }

        mid = fields[2];
        userId = fields[3];
        loginType = fields[4];
        request = fields[5];
        method = request.split(&quot; &quot;)[0];
        requestUrl = request.split(&quot; &quot;)[1];
        httpVersion = request.split(&quot; &quot;)[2];

        status = fields[6];
        httpReferer = fields[7];
        userAgent = fields[8];
        if (userAgent != null) {
            UserAgent uAgent = UserAgentUtil.getUserAgent(userAgent);
            if (uAgent != null) {
                browser = uAgent.getBrowserType();
            }
        }
        try { // 转换有可能出现异常
            time = df.format(new Date(Long.parseLong(fields[9])));
        } catch (NumberFormatException e) {
            logger.error(e.getMessage());
        }
        AccessLogWritable access = new AccessLogWritable(appid, ip, province, city, mid,
                userId, loginType, request, method, requestUrl,
                httpVersion, status, httpReferer, this.userAgent, browser, time);
        context.write(NullWritable.get(), new Text(access.toString()));
    }

    @Override
    protected void cleanup(Context context) throws IOException, InterruptedException {
        // 资源释放
        logger = null;
        df = null;
        JedisUtil.returnJedis(jedis);
    }
}</code></pre><p>AccessLogCleanJob  </p>
<pre><code>package cn.xpleaf.dataClean.mr.job;

import cn.xpleaf.dataClean.mr.mapper.AccessLogCleanMapper;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

/**
 * 清洗用户access日志信息
 * 主要的驱动程序
 *      主要用作组织mapper和reducer的运行
 *
 * 输入参数：
 * hdfs://ns1/input/data-clean/access/2018/04/08 hdfs://ns1/output/data-clean/access
 * 即inputPath和outputPath
 * 目前outputPath统一到hdfs://ns1/output/data-clean/access
 * 而inputPath则不确定，因为我们的日志采集是按天来生成一个目录的
 * 所以上面的inputPath只是清洗2018-04-08这一天的
 */
public class AccessLogCleanJob {
    public static void main(String[] args) throws Exception {

        if(args == null || args.length &lt; 2) {
            System.err.println(&quot;Parameter Errors! Usage &lt;inputPath...&gt; &lt;outputPath&gt;&quot;);
            System.exit(-1);
        }

        Path outputPath = new Path(args[args.length - 1]);

        Configuration conf = new Configuration();
        String jobName = AccessLogCleanJob.class.getSimpleName();
        Job job = Job.getInstance(conf, jobName);
        job.setJarByClass(AccessLogCleanJob.class);

        // 设置mr的输入参数
        for( int i = 0; i &lt; args.length - 1; i++) {
            FileInputFormat.addInputPath(job, new Path(args[i]));
        }
        job.setInputFormatClass(TextInputFormat.class);
        job.setMapperClass(AccessLogCleanMapper.class);
        job.setMapOutputKeyClass(NullWritable.class);
        job.setMapOutputValueClass(Text.class);
        // 设置mr的输出参数
        outputPath.getFileSystem(conf).delete(outputPath, true);    // 避免job在运行的时候出现输出目录已经存在的异常
        FileOutputFormat.setOutputPath(job, outputPath);
        job.setOutputFormatClass(TextOutputFormat.class);

        job.setOutputKeyClass(NullWritable.class);
        job.setOutputValueClass(Text.class);
        job.setNumReduceTasks(0);   // map only操作，没有reducer

        job.waitForCompletion(true);
    }
}</code></pre><h1 id="执行MapReduce程序"><a href="#执行MapReduce程序" class="headerlink" title="执行MapReduce程序"></a>执行MapReduce程序</h1><p>将上面的mr程序打包后上传到我们的Hadoop环境中，这里，对2018-04-08这一天产生的日志数据进行清洗，执行如下命令：  </p>
<pre><code>yarn jar data-extract-clean-analysis-1.0-SNAPSHOT-jar-with-dependencies.jar\
cn.xpleaf.dataClean.mr.job.AccessLogCleanJob \
hdfs://ns1/input/data-clean/access/2018/04/08 \
hdfs://ns1/output/data-clean/access</code></pre><p>观察结果，可以看到MapReduce Job执行成功！</p>
<pre><code>......
18/04/08 20:54:21 INFO mapreduce.Job: Running job: job_1523133033819_0009
18/04/08 20:54:28 INFO mapreduce.Job: Job job_1523133033819_0009 running in uber mode : false
18/04/08 20:54:28 INFO mapreduce.Job:  map 0% reduce 0%
18/04/08 20:54:35 INFO mapreduce.Job:  map 50% reduce 0%
18/04/08 20:54:40 INFO mapreduce.Job:  map 76% reduce 0%
18/04/08 20:54:43 INFO mapreduce.Job:  map 92% reduce 0%
18/04/08 20:54:45 INFO mapreduce.Job:  map 100% reduce 0%
18/04/08 20:54:46 INFO mapreduce.Job: Job job_1523133033819_0009 completed successfully
18/04/08 20:54:46 INFO mapreduce.Job: Counters: 31
......</code></pre><h1 id="数据清洗结果"><a href="#数据清洗结果" class="headerlink" title="数据清洗结果"></a>数据清洗结果</h1><p>上面的MapReduce程序执行成功后，可以看到在HDFS中生成的数据输出目录：<br><img src="/images/hadoop3.png"><br>我们可以下载其中一个结果数据文件，并用Notepadd++打开查看其数据信息：<br><img src="/images/hadoop4.png"></p>
<h1 id="数据处理："><a href="#数据处理：" class="headerlink" title="数据处理："></a>数据处理：</h1><p>对规整数据进行统计分析<br>经过数据清洗之后，就得到了我们做数据的分析统计所需要的比较规整的数据，下面就可以进行数据的统计分析了，即按照业务需求，统计出某一天中每个省份的PV和UV。  </p>
<p>我们依然是需要编写MapReduce程序，并且将数据保存到HDFS中，其架构跟前面的数据清洗是一样的：<br><img src="/images/hadoop5.png"></p>
<h1 id="数据处理思路："><a href="#数据处理思路：" class="headerlink" title="数据处理思路："></a>数据处理思路：</h1><p>如何编写MapReduce程序<br>现在我们已经得到了规整的数据，关于在于如何编写我们的MapReduce程序。  </p>
<p>因为要统计的是每个省对应的pv和uv，pv就是点击量，uv是独立访客量，需要将省相同的数据拉取到一起，拉取到一块的这些数据每一条记录就代表了一次点击（pv + 1），这里面有同一个用户产生的数据（通过mid来唯一地标识是同一个浏览器，用mid进行去重，得到的就是uv）。  </p>
<p>而拉取数据，可以使用Mapper来完成，对数据的统计（pv、uv的计算）则可以通过Reducer来完成，即Mapper的各个参数可以为如下： </p>
<pre><code>Mapper&lt;LongWritable, Text, Text(Province), Text(mid)&gt;</code></pre><p>而Reducer的各个参数可以为如下：</p>
<pre><code>Reducer&lt;Text(Province), Text(mid), Text(Province), Text(pv + uv)&gt;</code></pre><p>数据处理过程：MapReduce程序编写<br>根据前面的分析，来编写我们的MapReduce程序。<br>ProvincePVAndUVMapper</p>
<pre><code>package cn.xpleaf.dataClean.mr.mapper;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Mapper&lt;LongWritable, Text, Text(Province), Text(mid)&gt;
 * Reducer&lt;Text(Province), Text(mid), Text(Province), Text(pv + uv)&gt;
 */
public class ProvincePVAndUVMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] fields = line.split(&quot;\t&quot;);
        if(fields == null || fields.length != 16) {
            return;
        }
        String province = fields[2];
        String mid = fields[4];
        context.write(new Text(province), new Text(mid));
    }
}</code></pre><p>ProvincePVAndUVReducer</p>
<pre><code>package cn.xpleaf.dataClean.mr.reducer;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.HashSet;
import java.util.Set;

/**
 * 统计该标准化数据，产生结果
 * 省    pv      uv
 * 这里面有同一个用户产生的数|据（通过mid来唯一地标识是同一个浏览器，用mid进行去重，得到的就是uv）
 * Mapper&lt;LongWritable, Text, Text(Province), Text(mid)&gt;
 * Reducer&lt;Text(Province), Text(mid), Text(Province), Text(pv + uv)&gt;
 */
public class ProvincePVAndUVReducer extends Reducer&lt;Text, Text, Text, Text&gt; {

    private Set&lt;String&gt; uvSet = new HashSet&lt;&gt;();

    @Override
    protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {
        long pv = 0;
        uvSet.clear();
        for(Text mid : values) {
            pv++;
            uvSet.add(mid.toString());
        }
        long uv = uvSet.size();
        String pvAndUv = pv + &quot;\t&quot; + uv;
        context.write(key, new Text(pvAndUv));
    }
}</code></pre><p>ProvincePVAndUVJob</p>
<pre><code>package cn.xpleaf.dataClean.mr.job;

import cn.xpleaf.dataClean.mr.mapper.ProvincePVAndUVMapper;
import cn.xpleaf.dataClean.mr.reducer.ProvincePVAndUVReducer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

/**
 * 统计每个省的pv和uv值
 * 输入：经过clean之后的access日志
 *      appid ip province city mid userid login_type request method request_url http_version status http_referer user_agent browser yyyy-MM-dd HH:mm:ss
 * 统计该标准化数据，产生结果
 * 省    pv      uv
 *
 * 分析：因为要统计的是每个省对应的pv和uv
 *      pv就是点击量，uv是独立访客量
 *      需要将省相同的数据拉取到一起，拉取到一块的这些数据每一条记录就代表了一次点击（pv + 1）
 *      这里面有同一个用户产生的数据（通过mid来唯一地标识是同一个浏览器，用mid进行去重，得到的就是uv）
 *      Mapper&lt;LongWritable, Text, Text(Province), Text(mid)&gt;
 *      Reducer&lt;Text(Province), Text(mid), Text(Province), Text(pv + uv)&gt;
 *
 *  输入参数：
 *  hdfs://ns1/output/data-clean/access hdfs://ns1/output/pv-uv
 */
public class ProvincePVAndUVJob {
    public static void main(String[] args) throws Exception {

        if (args == null || args.length &lt; 2) {
            System.err.println(&quot;Parameter Errors! Usage &lt;inputPath...&gt; &lt;outputPath&gt;&quot;);
            System.exit(-1);
        }

        Path outputPath = new Path(args[args.length - 1]);

        Configuration conf = new Configuration();
        String jobName = ProvincePVAndUVJob.class.getSimpleName();
        Job job = Job.getInstance(conf, jobName);
        job.setJarByClass(ProvincePVAndUVJob.class);

        // 设置mr的输入参数
        for (int i = 0; i &lt; args.length - 1; i++) {
            FileInputFormat.addInputPath(job, new Path(args[i]));
        }
        job.setInputFormatClass(TextInputFormat.class);
        job.setMapperClass(ProvincePVAndUVMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        // 设置mr的输出参数
        outputPath.getFileSystem(conf).delete(outputPath, true);    // 避免job在运行的时候出现输出目录已经存在的异常
        FileOutputFormat.setOutputPath(job, outputPath);
        job.setOutputFormatClass(TextOutputFormat.class);
        job.setReducerClass(ProvincePVAndUVReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        job.setNumReduceTasks(1);

        job.waitForCompletion(true);
    }
}</code></pre><h1 id="执行MapReduce程序-1"><a href="#执行MapReduce程序-1" class="headerlink" title="执行MapReduce程序"></a>执行MapReduce程序</h1><p>将上面的mr程序打包后上传到我们的Hadoop环境中，这里，对前面预处理之后的数据进行统计分析，执行如下命令：</p>
<pre><code>yarn jar data-extract-clean-analysis-1.0-SNAPSHOT-jar-with-dependencies.jar \
cn.xpleaf.dataClean.mr.job.ProvincePVAndUVJob \
hdfs://ns1/output/data-clean/access \
hdfs://ns1/output/pv-uv</code></pre><p>观察运行结果、可以看到MapReduce Job执行成功！</p>
<pre><code>......
18/04/08 22:22:42 INFO mapreduce.Job: Running job: job_1523133033819_0010
18/04/08 22:22:49 INFO mapreduce.Job: Job job_1523133033819_0010 running in uber mode : false
18/04/08 22:22:49 INFO mapreduce.Job:  map 0% reduce 0%
18/04/08 22:22:55 INFO mapreduce.Job:  map 50% reduce 0%
18/04/08 22:22:57 INFO mapreduce.Job:  map 100% reduce 0%
18/04/08 22:23:03 INFO mapreduce.Job:  map 100% reduce 100%
18/04/08 22:23:03 INFO mapreduce.Job: Job job_1523133033819_0010 completed successfully
18/04/08 22:23:03 INFO mapreduce.Job: Counters: 49
......</code></pre><h1 id="数据处理结果"><a href="#数据处理结果" class="headerlink" title="数据处理结果"></a>数据处理结果</h1><p>上面的MapReduce程序执行成功后，可以看到在HDFS中生成的数据输出目录：<br><img src="/images/hadoop6.png"><br>我们可以下载其结果数据文件，并用Notepadd++打开查看其数据信息：<br><img src="/images/hadoop7.png"><br>至此，就完成了一个完整的数据采集、清洗、处理的完整离线数据分析案例。</p>
<p>相关的代码留言邮箱我Email你，有兴趣可以参考一下</p>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:Likj128@126.com">Alexis (KangJ)</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2020/06/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E3%80%81%E6%B8%85%E6%B4%97%E3%80%81%E5%A4%84%E7%90%86%EF%BC%9A%E4%BD%BF%E7%94%A8MapReduce%E8%BF%9B%E8%A1%8C%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A4%A7%E6%95%B0%E6%8D%AE/">https://programmer996.club/2020/06/01/大数据采集、清洗、处理：使用MapReduce进行离线数据分析-大数据/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://programmer996.club">Alexis (KangJ)的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tag"></i><a href="/tags/Java%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">Java大数据开发</a></span></div><aside id="toc"><div class="toc-title">目录</div><nav><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#大数据处理的常用方法"><span class="toc-number">1.</span> <span class="toc-text">大数据处理的常用方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#生产场景与需求"><span class="toc-number">2.</span> <span class="toc-text">生产场景与需求</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据采集：获取原生数据"><span class="toc-number">3.</span> <span class="toc-text">数据采集：获取原生数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗："><span class="toc-number">4.</span> <span class="toc-text">数据清洗：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗目的"><span class="toc-number">5.</span> <span class="toc-text">数据清洗目的</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗方案"><span class="toc-number">6.</span> <span class="toc-text">数据清洗方案</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗过程："><span class="toc-number">7.</span> <span class="toc-text">数据清洗过程：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#执行MapReduce程序"><span class="toc-number">8.</span> <span class="toc-text">执行MapReduce程序</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗结果"><span class="toc-number">9.</span> <span class="toc-text">数据清洗结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据处理："><span class="toc-number">10.</span> <span class="toc-text">数据处理：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据处理思路："><span class="toc-number">11.</span> <span class="toc-text">数据处理思路：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#执行MapReduce程序-1"><span class="toc-number">12.</span> <span class="toc-text">执行MapReduce程序</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据处理结果"><span class="toc-number">13.</span> <span class="toc-text">数据处理结果</span></a></li></ol></nav></aside></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="next" href="/2020/06/01/Spark%E7%94%9F%E6%80%81%E5%9C%88-%E5%A4%A7%E6%95%B0%E6%8D%AE/">Spark生态圈---大数据 &gt;</a></div><div id="valine-comment"><style type="text/css">.v * { color: #CECECE; }
.v a { color: #0F9FB4; }
.v a:hover { color: #216C73; }
.v li { list-style: inherit; }
.v .vwrap { border: 1px solid #223441; border-radius: 0; }
.v .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.v .vbtn { border-radius: 0; color: #cecece; background: none; }
.v .vlist .vcard .vh { border-bottom-color: #293D4E; }
.v .vwrap .vheader .vinput { border-bottom-color: #223441; }
.v .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.v code, .v pre,.v .vlist .vcard .vhead .vsys { background: #203240; }
.v code, .v pre { color: #F0F0F0; font-size: 95%; }
.v .vlist .vcard .vcontent.expand:before { background: linear-gradient(180deg,hsla(206,33%,19%,0),hsla(206,33%,19%,.9)); }
.v .vlist .vcard .vcontent.expand:after { background: hsla(206,33%,19%,.9); }</style><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'99FfVECWi1xns2SAjWOtTNza-gzGzoHsz',
  appKey:'6t7I76O3iToh6BGjc5rjWqDX',
  lang: 'zh-cn',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2020 - 2020 <a href="/." rel="nofollow">艾利克斯工作室</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Personal by<a rel="nofollow" target="_blank" href="https://programmer996.club"> Alexis.</a>Powered by<a rel="nofollow" target="_blank" href="http://www.beianbeian.com/beianxinxi/f72222db0e8cf7f7b49ceee25f65c49e.html"> 晋ICP备19013202号.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"></body></html>